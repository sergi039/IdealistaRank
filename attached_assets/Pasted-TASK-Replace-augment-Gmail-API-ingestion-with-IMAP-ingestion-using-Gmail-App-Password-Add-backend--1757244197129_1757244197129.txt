TASK: Replace/augment Gmail API ingestion with IMAP ingestion using Gmail App Password. Add backend switch via ENV. Keep UI in English.

CHANGES:
1) Add env toggles and secrets
2) Create services/imap_service.py
3) Wire scheduler to chosen backend (IMAP vs Gmail API)
4) Make /ingest/email/run call the active backend
5) Add unit tests for IMAP path
6) Update README and .env.example
7) Add dependency: imapclient

FILES TO ADD OR MODIFY:

--- add to requirements.txt ---
imapclient==3.0.1

--- update config.py ---
# add:
EMAIL_BACKEND = os.getenv("EMAIL_BACKEND", "imap").lower()  # 'imap' or 'gmail'
IMAP_HOST = os.getenv("IMAP_HOST", "imap.gmail.com")
IMAP_PORT = int(os.getenv("IMAP_PORT", "993"))
IMAP_SSL = os.getenv("IMAP_SSL", "true").lower() == "true"
IMAP_USER = os.getenv("IMAP_USER", "")
IMAP_PASSWORD = os.getenv("IMAP_PASSWORD", "")
IMAP_FOLDER = os.getenv("IMAP_FOLDER", "Idealista")  # Gmail label mapped as folder
IMAP_SEARCH_QUERY = os.getenv("IMAP_SEARCH_QUERY", "ALL")  # e.g. 'UNSEEN' or date filters
MAX_EMAILS_PER_RUN = int(os.getenv("MAX_EMAILS_PER_RUN", "200"))

--- create services/imap_service.py ---
from imapclient import IMAPClient
from email import message_from_bytes
from typing import List, Dict, Any, Tuple
from datetime import datetime, timezone
import hashlib
from app.config import (
    IMAP_HOST, IMAP_PORT, IMAP_SSL, IMAP_USER, IMAP_PASSWORD,
    IMAP_FOLDER, IMAP_SEARCH_QUERY, MAX_EMAILS_PER_RUN
)
from app.storage.db import SessionLocal
from app.services.scoring_service import score_listing_if_needed
from app.utils.email_parser import extract_listings_from_html   # already exists
from app.models import Listing, IngestionLog, Setting
from sqlalchemy import select
import logging
log = logging.getLogger(__name__)

def _get_setting(db, key: str, default: str = "") -> str:
    s = db.execute(select(Setting).where(Setting.key == key)).scalar_one_or_none()
    return s.value if s else default

def _set_setting(db, key: str, value: str) -> None:
    s = db.execute(select(Setting).where(Setting.key == key)).scalar_one_or_none()
    if s:
        s.value = value
    else:
        s = Setting(key=key, value=value)
        db.add(s)

def _html_parts(msg) -> List[bytes]:
    parts = []
    if msg.is_multipart():
        for p in msg.walk():
            ct = p.get_content_type()
            if ct == "text/html":
                parts.append(p.get_payload(decode=True) or b"")
    else:
        if msg.get_content_type() == "text/html":
            parts.append(msg.get_payload(decode=True) or b"")
    return parts

def run_imap_ingest() -> Dict[str, Any]:
    """Main IMAP polling job; returns stats dict."""
    started = datetime.now(timezone.utc)
    stats = {"found": 0, "new": 0, "updated": 0}
    run_id = started.strftime("imap-%Y%m%d-%H%M%S")
    db = SessionLocal()
    try:
        last_seen_uid_key = "imap:last_seen_uid"
        last_seen_uid = _get_setting(db, last_seen_uid_key, "0")
        last_uid_int = int(last_seen_uid or "0")

        with IMAPClient(IMAP_HOST, port=IMAP_PORT, ssl=IMAP_SSL) as client:
            client.login(IMAP_USER, IMAP_PASSWORD)
            # Select folder/label (Gmail maps labels to folders)
            client.select_folder(IMAP_FOLDER, readonly=True)

            # Search only new UIDs beyond last_seen_uid
            search_criteria = IMAP_SEARCH_QUERY
            uids = client.search(search_criteria)  # returns list of ints
            if last_uid_int > 0:
                uids = [u for u in uids if u > last_uid_int]
            uids = sorted(uids)[:MAX_EMAILS_PER_RUN]

            stats["found"] = len(uids)
            if not uids:
                _set_setting(db, last_seen_uid_key, str(last_uid_int))
                db.commit()
                _log_ingestion(db, run_id, started, "ok", stats, "no new emails")
                return stats

            fetch_data = client.fetch(uids, ["RFC822", "INTERNALDATE"])
            for uid in uids:
                raw = fetch_data[uid][b"RFC822"]
                msg = message_from_bytes(raw)
                internal_date = fetch_data[uid][b"INTERNALDATE"]
                received_at = datetime.fromtimestamp(internal_date.timestamp(), tz=timezone.utc)

                htmls = _html_parts(msg)
                if not htmls:
                    continue

                for html in htmls:
                    html_hash = hashlib.sha256(html).hexdigest()
                    # parse Idealista cards from this HTML block
                    listings = extract_listings_from_html(html.decode("utf-8", errors="ignore"))
                    for item in listings:
                        # expected keys from parser: url,title,price_eur,plot_area_sqm,location_text,listing_type,land_type,descr_text,external_id
                        _upsert_listing(db, item, received_at, html_hash, stats)

                last_uid_int = max(last_uid_int, int(uid))

            # persist last seen UID
            _set_setting(db, last_seen_uid_key, str(last_uid_int))
            db.commit()

        _log_ingestion(db, run_id, started, "ok", stats, f"Processed {len(uids)} emails")
        return stats
    except Exception as e:
        log.exception("IMAP ingest failed")
        _log_ingestion(db, run_id, started, "error", stats, str(e))
        raise
    finally:
        db.close()

def _upsert_listing(db, item: Dict[str, Any], received_at: datetime, html_hash: str, stats: Dict[str, int]):
    # Find or create by (source, external_id) or fallback hash
    source = "idealista_email"
    external_id = item.get("external_id") or hashlib.sha1(
        f"{item.get('url','')}|{item.get('title','')}|{item.get('price_eur','')}|{item.get('plot_area_sqm','')}".encode()
    ).hexdigest()

    existing = db.query(Listing).filter_by(source=source, external_id=external_id).one_or_none()
    values = dict(
        source=source,
        external_id=external_id,
        url=item.get("url"),
        title=item.get("title"),
        listing_type=item.get("listing_type"),
        land_type=item.get("land_type"),  # 'developed'|'buildable' or None
        price_eur=item.get("price_eur"),
        plot_area_sqm=item.get("plot_area_sqm"),
        price_per_sqm_eur=(item.get("price_eur") / item.get("plot_area_sqm")) if (item.get("price_eur") and item.get("plot_area_sqm")) else None,
        location_text=item.get("location_text"),
        municipality=item.get("municipality"),
        district=item.get("district"),
        descr_text=item.get("descr_text"),
        email_received_at=received_at,
        added_at=item.get("added_at") or received_at,
        scraped_at=received_at,
        raw_email_hash=html_hash,
    )
    if existing:
        for k, v in values.items():
            setattr(existing, k, v)
        stats["updated"] += 1
        db.flush()
        score_listing_if_needed(db, existing.id)
    else:
        rec = Listing(**values)
        db.add(rec)
        db.flush()
        stats["new"] += 1
        score_listing_if_needed(db, rec.id)

def _log_ingestion(db, run_id, started_at, status, stats, message):
    db.add(IngestionLog(
        source="idealista_email",
        run_id=run_id, status=status, items_found=stats.get("found",0),
        items_new=stats.get("new",0), items_updated=stats.get("updated",0),
        started_at=started_at, finished_at=datetime.now(timezone.utc), message=message
    ))
    db.commit()

--- update services/scheduler_service.py ---
# after loading config:
from app.config import EMAIL_BACKEND, TZ
from app.services.imap_service import run_imap_ingest
from app.services.gmail_service import run_gmail_ingest  # keep existing
def ingest_job():
    if EMAIL_BACKEND == "imap":
        return run_imap_ingest()
    return run_gmail_ingest()
# cron remains the same (08:00 and 20:00 in TZ)

--- update routes/api_routes.py ---
@api_router.post("/ingest/email/run")
def ingest_email_now():
    from app.config import EMAIL_BACKEND
    if EMAIL_BACKEND == "imap":
        from app.services.imap_service import run_imap_ingest
        return run_imap_ingest()
    else:
        from app.services.gmail_service import run_gmail_ingest
        return run_gmail_ingest()

--- update .env.example ---
# choose backend: 'imap' (preferred) or 'gmail'
EMAIL_BACKEND=imap

# IMAP (Gmail App Password)
IMAP_HOST=imap.gmail.com
IMAP_PORT=993
IMAP_SSL=true
IMAP_USER=your@gmail.com
IMAP_PASSWORD=your_app_password
IMAP_FOLDER=Idealista
IMAP_SEARCH_QUERY=ALL
MAX_EMAILS_PER_RUN=200

# keep existing keys (DB, scoring, tz, etc.)

--- update README.md (snippet) ---
### IMAP (Gmail App Password) — recommended
1. Enable 2FA in your Google Account, then create an **App Password** (type: Mail).
2. In Gmail, create a filter for Idealista alerts: Skip Inbox + Apply label **Idealista**.
3. Set Replit Secrets:
   - EMAIL_BACKEND=imap
   - IMAP_HOST=imap.gmail.com
   - IMAP_PORT=993
   - IMAP_SSL=true
   - IMAP_USER=you@gmail.com
   - IMAP_PASSWORD=<app password>
   - IMAP_FOLDER=Idealista
4. Click **Run** and visit `POST /ingest/email/run` to import immediately. The scheduler runs 08:00 & 20:00 (TZ).

Security: read-only access, we never delete or mark emails. App password can be revoked anytime in Google Account → Security.

--- add tests/tests/test_imap_service.py ---
- mock IMAPClient responses (UID list + RFC822 payload)
- feed sample HTML (Idealista newsletter block)
- assert: parsed listings saved, dedup by (source, external_id), scoring invoked, last_seen_uid updated in settings
